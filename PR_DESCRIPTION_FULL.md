# Ollama NLP Backend for Custom Computed Fields - Full PR Description

## Overview
This PR adds a complete local LLM-powered backend service for generating SQL expressions for custom computed fields in Graphic Walker using Ollama. The service runs entirely on-premises with no external API dependencies, ensuring privacy, cost-efficiency, and offline capability.

**ðŸ“¹ Includes demonstration video: [NLP-Based Compute Field](./packages/nlp-backend/NLP-Based-Compute-Field.mp4)**

## Purpose
Enable users to generate SQL computed field expressions from natural language descriptions using their local Ollama installation, providing a seamless integration between Graphic Walker's UI and local language models.

## Key Features Implemented

### 1. **Natural Language to SQL Conversion**
- Local LLM processing via Ollama integration
- Intelligent prompt engineering for proper SQL generation
- Support for conditional logic (CASE WHEN...THEN...ELSE...END)
- Field-aware SQL expression generation
- Mathematical and arithmetic operations support

### 2. **Advanced SQL Processing**
- **Comprehensive SQL processor** (`sql_processor.py`):
  - Markdown and code block removal
  - Ollama-specific response pattern cleaning
  - Model hallucination detection and filtering
  - SQL validation and syntax checking
  - Complex expression extraction from mixed content
  - Configurable regex patterns for maximum flexibility

### 3. **Robust Service Architecture**
- **Ollama client** (`ollama_client.py`):
  - Async/await support for non-blocking operations
  - Model availability validation and fallback logic
  - Graceful error handling with specific error categorization
  - Health checking with timeout protection
  
- **Service layer** (`ollama_service.py`):
  - Request processing with comprehensive logging
  - Error categorization (timeout, connection, model, validation errors)
  - Detailed request tracking and monitoring
  
- **Model management** (`model_manager.py`):
  - Dynamic model selection and validation
  - Fallback model support
  - Model availability caching
  - Recommended models identification

### 4. **Health Monitoring & Observability**
- **Health monitoring** (`health_monitor.py`):
  - Comprehensive startup validation
  - Multi-level health checks (connectivity, model availability, performance, resources)
  - Health status tracking and history
  - Degraded state detection

- **Metrics collection** (`metrics_collector.py`):
  - Request metrics tracking (response times, success rates, error types)
  - Performance time series data collection
  - Request history with filtering
  - Detailed performance reports and trend analysis
  - Thread-safe metrics collection

- **Monitoring dashboard** (`monitoring_dashboard.py`):
  - Real-time metrics aggregation
  - Performance trend analysis
  - Health status formatting
  - Report generation (hourly, daily, weekly)
  - Alert system for anomalies

### 5. **Enterprise-Grade Logging**
- **Structured logging** (`logging_config.py`):
  - JSON-formatted logs for log aggregation
  - Contextual logging with request tracing
  - Error categorization and detailed error logging
  - Multiple output handlers (console, file)
  - Configurable log levels and formats

### 6. **FastAPI Integration**
- **Complete REST API** (`app.py`):
  - Text-to-SQL endpoint with model override support
  - Detailed SQL generation endpoint with processing info
  - Raw SQL response processing endpoint
  - Comprehensive health check endpoints
  - Service information and model listing endpoints
  - Full monitoring dashboard endpoints
  - Metrics export endpoints (JSON, Prometheus formats)

## Technical Implementation Details

### API Endpoints

#### Primary Endpoints
- `POST /api/ollama-text2sql` - Basic SQL generation from natural language
- `POST /api/ollama-text2sql/detailed` - SQL generation with detailed processing information
- `POST /api/sql/process` - Process raw SQL responses for testing/debugging

#### Health & Monitoring Endpoints
- `GET /api/health` - Basic health check
- `GET /api/health/detailed` - Comprehensive health status
- `GET /api/health/metrics` - Health metrics and statistics
- `GET /api/health/history` - Recent health check history
- `GET /api/service-info` - Service configuration and status
- `GET /api/models` - Available models from Ollama

#### Monitoring & Analytics Endpoints
- `GET /api/monitoring/dashboard` - Full monitoring dashboard data
- `GET /api/monitoring/real-time` - Real-time metrics
- `GET /api/monitoring/health-status` - Health status for monitoring systems
- `GET /api/monitoring/performance-trends` - Performance trends (configurable period)
- `GET /api/monitoring/report/{report_type}` - Comprehensive reports
- `GET /api/metrics/summary` - Metrics summary
- `GET /api/metrics/export?format=json|prometheus` - Metrics export
- `GET /api/metrics/time-series/{metric_name}` - Time series data
- `GET /api/metrics/requests` - Request history

### Configuration
Environment variables for flexibility:
- `OLLAMA_BASE_URL` - Ollama server URL (default: http://localhost:11434)
- `OLLAMA_MODEL` - Primary model (default: codellama:7b-instruct)
- `OLLAMA_FALLBACK_MODEL` - Fallback model (default: llama3.2:latest)
- `OLLAMA_TIMEOUT` - Request timeout in seconds (default: 90)
- `LOG_LEVEL` - Logging level (default: INFO)
- `LOG_FORMAT` - Log format: json or text (default: json)
- `LOG_FILE` - Optional log file path
- `METRICS_RETENTION_HOURS` - Metrics retention period (default: 24)
- `MAX_POINTS_PER_METRIC` - Maximum metrics data points (default: 10000)

### SQL Processing Pipeline

1. **Input Processing**: Validate and prepare raw response
2. **Markdown Removal**: Strip code blocks and formatting
3. **Ollama Pattern Cleaning**: Remove model-specific response patterns
4. **Whitespace Cleanup**: Normalize formatting and spacing
5. **Validation & Extraction**: Validate SQL keywords and extract core expression
6. **Metadata Generation**: Create processing metadata and complexity estimation

### Error Handling Strategy

**Error Categories:**
- `TIMEOUT_ERROR` - Request exceeded timeout
- `CONNECTION_ERROR` - Cannot connect to Ollama
- `MODEL_ERROR` - Model unavailable or generation failed
- `VALIDATION_ERROR` - Invalid input
- `CONFIGURATION_ERROR` - Configuration issues
- `UNKNOWN_ERROR` - Other errors

**Fallback Mechanism:**
1. Try primary model
2. On failure, automatically attempt fallback model
3. Return error only if both fail

## Files Added

### Core Service Files
- `app.py` - FastAPI application with all endpoints
- `config.py` - Configuration management
- `ollama_client.py` - Ollama integration client
- `ollama_service.py` - Service layer

### Processing & Validation
- `sql_processor.py` - Advanced SQL processing and cleaning
- `model_manager.py` - Model selection and validation

### Monitoring & Observability
- `health_monitor.py` - Health checking and monitoring
- `metrics_collector.py` - Metrics collection and analytics
- `monitoring_dashboard.py` - Dashboard data aggregation
- `logging_config.py` - Structured logging configuration

### Documentation & Configuration
- `requirements.txt` - Python dependencies
- `README.md` - Quick start guide
- `OLLAMA_NLP_BACKEND_GUIDE.md` - Comprehensive guide
- `.env.example` - Environment configuration template

## Dependencies

```
fastapi - Web framework
uvicorn - ASGI server
python-dotenv - Environment configuration
ollama - Ollama Python client
psutil - System resource monitoring
pytest - Testing framework
pytest-asyncio - Async testing
pytest-cov - Coverage reporting
```

## Testing

The implementation includes comprehensive testing capabilities:
- Async operation support with pytest-asyncio
- Mock health checks and model availability
- Edge case handling (empty prompts, malformed responses)
- Error categorization and fallback testing
- Metrics collection validation
- Performance benchmarking support

## Deployment Considerations

### Prerequisites
1. Ollama server running (default: localhost:11434)
2. At least one model pulled (recommended: codellama:7b-instruct)
3. Python 3.8+

### Quick Start
```bash
cd packages/nlp-backend
pip install -r requirements.txt
uvicorn app:app --reload --port 3002
```

### Production Deployment
- Use production ASGI server (Gunicorn + Uvicorn)
- Configure logging to file
- Set up monitoring/alerting
- Use environment variables for configuration
- Implement load balancing if needed

## Performance Characteristics

- **Typical Response Time**: 3-15 seconds (model-dependent)
- **Fallback Logic**: <1 second additional delay
- **Health Check Overhead**: <10ms
- **Metrics Collection**: Lock-based thread-safe implementation
- **Memory Usage**: ~50-200MB base + model size

## Security Considerations

1. **SQL Injection Prevention**:
   - Basic pattern detection for dangerous keywords
   - User disclaimer for custom expressions
   - SQL validation before output

2. **Local Processing**:
   - No external API calls
   - Data remains on user's infrastructure
   - No cloud dependencies

3. **Input Validation**:
   - Prompt length checking
   - Empty input rejection
   - Response validation

## Backward Compatibility
- Service is new, no breaking changes to existing code
- Can be optionally enabled/disabled in UI
- Falls back to manual SQL entry if service unavailable

## Documentation Provided
- Comprehensive setup guide with platform-specific instructions
- API documentation for all endpoints
- Configuration reference
- Usage examples
- Troubleshooting section

## Demonstration

### Video Walkthrough
A comprehensive demonstration of the Ollama NLP Backend for Custom Computed Fields:

<video width="100%" controls>
  <source src="./packages/nlp-backend/NLP-Based-Compute-Field.mp4" type="video/mp4">
  Your browser does not support the video tag. <a href="./packages/nlp-backend/NLP-Based-Compute-Field.mp4">Download video</a>
</video>

**[ðŸ“¹ Download: NLP-Based Compute Field](./packages/nlp-backend/NLP-Based-Compute-Field.mp4)** *(Right-click â†’ Save link as... to download)*

The video covers:
- âœ… Service startup and initialization
- âœ… Health check system in action
- âœ… Natural language to SQL conversion examples
- âœ… Fallback model mechanism
- âœ… Error handling and recovery
- âœ… Monitoring dashboard overview
- âœ… Metrics collection and analysis
- âœ… Real-world usage scenarios

### Example Conversions

**Example 1: Conditional Logic**
```
Prompt: "If revenue > 1000 then High else Low"
Generated SQL: CASE WHEN revenue > 1000 THEN 'High' ELSE 'Low' END
```

**Example 2: Mathematical Operation**
```
Prompt: "Calculate 10% of price"
Generated SQL: price * 0.1
```

**Example 3: Complex Condition**
```
Prompt: "If status is active then Premium else Basic"
Generated SQL: CASE WHEN status = 'active' THEN 'Premium' ELSE 'Basic' END
```

**Example 4: Multi-field Logic**
```
Prompt: "If revenue > cost then profit else loss"
Generated SQL: CASE WHEN revenue > cost THEN 'profit' ELSE 'loss' END
```

## Related Issues
- Enables custom computed fields feature in Graphic Walker
- Provides foundation for AI-powered UI enhancements

## Testing Checklist
- âœ… Service startup and initialization
- âœ… Health check endpoints
- âœ… Basic SQL generation
- âœ… Model fallback on failure
- âœ… Error categorization
- âœ… Metrics collection
- âœ… Request history tracking
- âœ… Monitoring dashboard data
- âœ… Different model types
- âœ… Edge cases (empty input, timeouts, etc.)

## Notes for Reviewers

1. **SQL Processing**: The regex-based cleaning pipeline is designed to handle various Ollama model output formats
2. **Async Implementation**: Uses thread pool for sync Ollama client operations
3. **Health Monitoring**: Comprehensive but can be extended for specific infrastructure needs
4. **Metrics**: Thread-safe collection with configurable retention
5. **Logging**: Structured JSON format for easy log aggregation
